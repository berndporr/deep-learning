<!DOCTYPE HTML>

<!--Converted with LaTeX2HTML 2021.2 (Released July 1, 2021) -->
<HTML lang="en">
<HEAD>
<TITLE>Non-linear networks</TITLE>
<META NAME="description" CONTENT="Non-linear networks">
<META NAME="keywords" CONTENT="deep-learning-analytics">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="viewport" CONTENT="width=device-width, initial-scale=1.0">
<META NAME="Generator" CONTENT="LaTeX2HTML v2021.2">

<LINK REL="STYLESHEET" HREF="deep-learning-analytics.css">

<LINK REL="next" HREF="node8.html">
<LINK REL="previous" HREF="node6.html">
<LINK REL="next" HREF="node8.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="deep-learning-analytics.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A>   
<BR>
<B> Next:</B> <A
 HREF="node8.html">References</A>
<B> Up:</B> <A
 HREF="deep-learning-analytics.html">Deep learning</A>
<B> Previous:</B> <A
 HREF="node6.html">Multi-layer network: error backpropagation</A>
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H1><A ID="SECTION00070000000000000000">
Non-linear networks</A>
</H1>
Only if computations in the separate neurons are non-linear
deeper networks make sense. Any linear deep network could always be
reduced to a single layer network rendering any deeper network
pointless. Consequently, we need to introduce now non-linearities.
Fig.&nbsp;<A HREF="node6.html#nonlin">3</A> shows a non-linear neuron which is the central building block
of any deep network where after the linear summation we introduce a non-linearity <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.91ex; vertical-align: -0.09ex; " SRC="img51.svg"
 ALT="$\Theta$"></SPAN>
which we call activation function:
<P></P>
<DIV CLASS="displaymath"><A ID="nonlinear_sum"></A><!-- MATH
 \begin{equation}
y_i(n) = \Theta\left(\underbrace{\sum_j y_j(n) w_{ji}}_{v_i} \right)
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG
 STYLE="height: 13.96ex; vertical-align: -6.41ex; " SRC="img52.svg"
 ALT="$\displaystyle y_i(n) = \Theta\left(\underbrace{\sum_j y_j(n) w_{ji}}_{v_i} \right)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">19</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
The crucial question is if the introduction of the activation function
changes the learning rule. Luckily not much.
Looking at Eq.&nbsp;<A HREF="node5.html#chainrule">10</A> it becomes directly clear that the
chain rule is simply expanded by another term:
 <P></P>
<DIV CLASS="displaymath"><!-- MATH
 \begin{equation}
\frac{\partial E}{\partial \omega_{ji}}  = \frac{\partial E}{\partial y_i} \underbrace{\frac{\partial y_i}{\partial v_i}}_{\Theta^\prime} \frac{\partial v_i}{\partial \omega_{ji}}
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG
 STYLE="height: 8.36ex; vertical-align: -5.02ex; " SRC="img53.svg"
 ALT="$\displaystyle \frac{\partial E}{\partial \omega_{ji}} = \frac{\partial E}{\part...
...l y_i}{\partial v_i}}_{\Theta^\prime} \frac{\partial v_i}{\partial \omega_{ji}}$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">20</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
where <!-- MATH
 $\Theta^\prime$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.02ex; vertical-align: -0.09ex; " SRC="img54.svg"
 ALT="$\Theta^\prime$"></SPAN> is simply the derivative of the activation function <SPAN CLASS="MATH"><IMG
 STYLE="height: 1.91ex; vertical-align: -0.09ex; " SRC="img51.svg"
 ALT="$\Theta$"></SPAN>. Consequently the
 weight change at the output is now calculated as:
<P></P>
<DIV CLASS="displaymath"><!-- MATH
 \begin{equation}
\Delta\omega_{ji} = \mu \cdot \Theta^\prime(y_i) \cdot y_j \cdot e_i
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG
 STYLE="height: 2.76ex; vertical-align: -0.72ex; " SRC="img55.svg"
 ALT="$\displaystyle \Delta\omega_{ji} = \mu \cdot \Theta^\prime(y_i) \cdot y_j \cdot e_i$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">21</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
and the internal error as:
<P></P>
<DIV CLASS="displaymath"><!-- MATH
 \begin{equation}
\Delta\omega_{kj} = \mu \cdot \Theta^\prime(y_j) \cdot y_k \cdot \underbrace{\sum_i e_i \omega_{ji}}_{e_j}
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG
 STYLE="height: 8.95ex; vertical-align: -6.34ex; " SRC="img56.svg"
 ALT="$\displaystyle \Delta\omega_{kj} = \mu \cdot \Theta^\prime(y_j) \cdot y_k \cdot \underbrace{\sum_i e_i \omega_{ji}}_{e_j}$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">22</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
Strictly, the activation function needs to be differentiable. However,
it turned out that the one way rectifier (Rectifiying Linear Unit = ReLU)
works extremely well (<A
 HREF="node8.html#Fukushima1975">Fukushima, 1975</A>):
<P></P>
<DIV CLASS="displaymath"><!-- MATH
 \begin{equation}
\Theta(v) =
  \begin{cases}
    0, & \text{if}\ v < 0 \\
    v, & \text{otherwise}
  \end{cases}
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG
 STYLE="height: 8.50ex; " SRC="img57.svg"
 ALT="\begin{displaymath}\Theta(v) =
\begin{cases}
0, &amp; \text{if}\ v &lt; 0 \\
v, &amp; \text{otherwise}
\end{cases}\end{displaymath}"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">23</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
Its derivative has no definite solution at its origin so that one needs
to decide if the derivative is zero there or one.
Other popular activation functions are <!-- MATH
 $\Theta(v)=\tanh(v)$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 2.62ex; vertical-align: -0.66ex; " SRC="img58.svg"
 ALT="$\Theta(v)=\tanh(v)$"></SPAN> or <!-- MATH
 $\Theta(v)=\frac{1}{1+e^{-v}}$
 -->
<SPAN CLASS="MATH"><IMG
 STYLE="height: 3.15ex; vertical-align: -1.03ex; " SRC="img59.svg"
 ALT="$\Theta(v)=\frac{1}{1+e^{-v}}$"></SPAN>.

<P>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="deep-learning-analytics.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A>   
<BR>
<B> Next:</B> <A
 HREF="node8.html">References</A>
<B> Up:</B> <A
 HREF="deep-learning-analytics.html">Deep learning</A>
<B> Previous:</B> <A
 HREF="node6.html">Multi-layer network: error backpropagation</A></DIV>
<!--End of Navigation Panel-->
<ADDRESS>
<p><br /><a href="https://github.com/berndporr/deep-learning-analytics">github / contact</a><br /></p>
</ADDRESS>
</BODY>
</HTML>
