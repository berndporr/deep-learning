\documentclass[12pt]{article}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[german]{babel}
\hypersetup{%
    pdfborder = {0 0 0}
}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\captionfont}{\small}

\author{Bernd Porr}
\title{Induktives Lernen anhand neuronaler Netze}

\begin{document}

\maketitle

\section{Induktives lernen}
Bekannt ist ein Datensatz $\vec{x}_1, \ldots, \vec{x}_N$, der mit einer unbekannten Funktion
\begin{equation}
  \vec{y}=f(\vec{x}) \label{inductive}
\end{equation}
in einen Datensatz $\vec{y}$
überführt wird. Das Ziel ist es, die Funktion $f$ iterativ mit Hilfe von Beispielen $\vec{x},\vec{y}$ zu lernen, so dass dann
neue $\vec{x}_{N+1}, \ldots$ korrekt in $\vec{y}=f(\vec{x})$ überführt werden.

In den folgenden Abschnitten wird die Funktion $f$ durch immer komplexere neuronale Netzwerke gelernt und approximiert.

\section{Lineares Neuron}

\begin{figure}[!hbt]
\begin{center}
\mbox{\includegraphics[width=\textwidth]{one_layer}}
\end{center}
\caption{Einfaches lineares Neuron
\label{one_layer}}
\end{figure}

Abb.~\ref{one_layer}A zeigt ein einfaches lineares Neuron in einer
einzigen Schicht. Die Aktivität des Neurons ist $y(n)$ und hat einen
Index für dessen Position in der Schicht, wobei der Index $i$ hier die
Ausgangsschicht repräsentiert. $n$ ist der Momentane Zeitpunkt als
Zeit-Index in einem getakteten System. Die Eingangsaktivität $x_j(n) =
y_j(n)$ wird mit $\omega_{ji}$ gewichtet und dann zum Ausgangssignal
$y_i(n)$ summiert:
\begin{equation}
  y_i(n) = \sum_j y_j(n) w_{ji} \label{linear_sum}
\end{equation}
Die Funktion $f$ ist hier also eine einfache lineare Kombination von den Eingangssignalen, wobei
die Gewichte $\omega_{ji}$ gelernt werden müssen, um die spezifische Funktion $f$ zu approximieren.

Gelernt werden soll eine bestimmte Ausgangsaktivität in Abhängigkeit von einer Eingangsaktivität (Gl.~\ref{inductive}).
Das wird erreicht, indem ein Fehler $e_i(n)$ am Ausgang jedes Neurons etabliert wird, der dann die Gewichte $w_{ji}$
schrittweise dorthin bewegt, so dass er im Mittel Null wird:
\begin{equation}
  e_i(n) = y_i(n) - d_i(n) \label{output_error}
\end{equation}
wobei $d_i(n)$ der gewünschte Ausgangswert ist und $y_i(n)$ der Aktuelle.
Ziel des Lernens ist es, dass das quadratische Mittel vom Fehler Null wird:
\begin{equation}
  E = \frac{1}{2} e^2 \label{quaderr}
\end{equation}

Der zentrale Trick ist nun, die Partielle Ableitung in Abhängigkeit vom Fehler $E$ zu nehmen
und damit die Gewichte zu verändern:
\begin{eqnarray}
  \Delta\omega_{ji} & = & - \mu \frac{\partial E}{\partial \omega_{ji}} \label{graddes} \\
  \omega_{ji} & \leftarrow & \omega_{ji} + \Delta\omega_{ji}
\end{eqnarray}
Warum macht das Sinn? In Abb.~\ref{one_layer}B ist die Abhängigkeit von einem Gewicht
$\omega_{ji}$ gezeigt. Wenn in diesem Beispiel das Gewicht etwas erhöht wird, dann
wird der Fehler $E$ größer. Also war die Erhöhung kontraproduktiv und es ist besser,
das Gewicht wieder zu verringern. Wenn aber umgekehrt eine Erhöhung von $\omega_{ji}$ eine
Verringerung von $E$ bewirkt, dann ist das Vorteilhaft also kann man das Gewicht erhoehen.
Iterativ wird also der Fehler $E$ verringert.

Als naechsten Schritt muss nun die Lernregel hergeleitet werden. Das geschieht, indem die
Gln.~\ref{linear_sum}, \ref{output_error} und \ref{quaderr} in Gl.~\ref{graddes} einsetzt werden:
\begin{eqnarray}
  \Delta\omega_{ji}
   & = & - \mu \frac{1}{2} \frac{\partial ( d_i(n) - y_i(n) )^2 }{\partial \omega_{ji}} \\
   & = & - \mu \frac{1}{2} \frac{\partial \left( d_i(n) - \sum_j y_j(n) w_{ji} \right)^2 }{\partial \omega_{ji}} \\
  & = & \mu \underbrace{\left(d_i(n) - \sum_j y_j(n) w_{ji}\right)}_{-e_i(n)} \cdot y_j(n) \\
   & = & - \mu \underbrace{\frac{\partial E}{\partial y_i}}_{-e_i(n)} \underbrace{\frac{\partial y_i}{\partial \omega_{ji}}}_{y_j(n)} \label{chainrule}\\
  & = & \mu \cdot e_i(n) \cdot y_j(n)
\end{eqnarray}
Die Lernregel ist also einfach die Multiplikation vom Fehler am Ausgang mit der Aktivität am Eingang. Dieses Produkt verändert
dann das zugehörige Gewicht.

\section{Mehrschichtiges lineares Netzwerk}
\begin{figure}[!hbt]
\begin{center}
\mbox{\includegraphics[width=\textwidth]{multi_layer}}
\end{center}
\caption{Mehrschichtiges Feedforward Netz
\label{multi_layer}}
\end{figure}

Abb.~\ref{multi_layer} zeigt nun ein Netzwerk mit mehreren
Schichten. Wie das Signal vom Eingang zum Ausgang durch die
verschiedenen Schichten läuft, ist einfach auszurechnen. Auch der
Fehler $e_i$ in der Ausgangsschicht kann weiterhin einfach als
Differenz zwischen $y_i$ und $d_i$ ausgerechnet werden (siehe
Gl.~\ref{output_error}). Das Problem ist, wie die \textsl{internen} Fehler
$e_j$ und $e_k$ ausgerechnet werden können.
Der interne Fehler für $\omega_{kj}$ ist erstmal genauso definiert wie der Fehler
am Ausgang (siehe Gl.~\ref{chainrule}):
\begin{equation}
  \frac{\partial E}{\partial \omega_{kj}} = \underbrace{\frac{\partial E}{\partial y_j}}_\textrm{Trick!} \frac{\partial y_j}{\partial \omega_{kj}}
  \label{gradint}
\end{equation}
aber der ist ja nicht direkt verfügbar. Der Trick ist nun den Term
$\frac{\partial E}{\partial y_j}$ mit Hilfe von der Aktivität $y_i$ am
Ausgang auszudrücken und dann einfach die beiden Terme mit
Gl.~\ref{linear_sum} und Gl.~\ref{chainrule} zu identifizieren:
\begin{equation}
  \frac{\partial E}{\partial y_j} = \sum_i \underbrace{\frac{\partial E}{\partial y_i}}_{-e_i} \cdot \underbrace{\frac{\partial y_i}{\partial y_j}}_{\omega_{ji}} \label{dltrick}
\end{equation}
Gl.~\ref{dltrick} wieder in Gl.~\ref{gradint} substituiert gibt dann:
\begin{equation}
  \frac{\partial E}{\partial \omega_{kj}} = - \left( \sum_i e_i \omega_{ji} \right) \frac{\partial y_j}{\partial \omega_{kj}}
  \label{gradintback}
\end{equation}
mit
\begin{equation}
y_j = \sum_k y_k(n) w_{kj}
\end{equation}
führt das zu:
\begin{eqnarray}
  \frac{\partial E}{\partial \omega_{kj}} & = & - \left( \sum_i e_i \omega_{ji} \right) \underbrace{\frac{\partial \left(\sum_k y_k(n) w_{kj}\right)}{\partial \omega_{kj}}}_{y_k} \\
                                         & = & - \left( \sum_i e_i \omega_{ji} \right) y_k
\end{eqnarray}
Die Änderung des internen Gewichtes $\omega_{kj}$ folgt dann folgender Gleichung:
\begin{equation}
\Delta\omega_{kj} = \mu \cdot y_k \cdot \underbrace{\sum_i e_i \omega_{ji}}_{e_j}
\end{equation}
Wobei nun $e_j$ der interne Error ist und damit ist es möglich, alle internen Fehler rückwärts vom Ausgang zum Eingang zu berechnen.

\begin{figure}[!hbt]
\begin{center}
\mbox{\includegraphics[width=0.5\textwidth]{nonlin}}
\end{center}
\caption{Neuron mit nichtlinearer Characteristik
\label{nonlin}}
\end{figure}

\section{Nicht-lineare Netzwerke}
Nur wenn Netzwerke nicht-linear sind, machen tiefere Schichten einen
Sinn, da jedes lineare Netzwerk immer auf eine Schicht reduziert
werden können.  Erst die Nichtlinearität rechtfertigt tiefe (``deep'')
mehrschichtige Netzwerke.  Abb.~\ref{nonlin} zeigt ein Neuron von
solch einem mehrschichtigen Netzwerk, wo die lineare Summe dann durch
eine Nichtlinearität geschickt wird:
\begin{equation}
  y_i(n) = \Theta\left(\underbrace{\sum_j y_j(n) w_{ji}}_{v_i} \right) \label{nonlinear_sum}
\end{equation}
Was aendert sich in diesem Fall an den Lernregeln? Sehr wenig. Wenn man sich Gl.~\ref{chainrule} ansieht, merkt man, dass einfach
ein weiterer Term durch die Kettenregel hinzukommt:
 \begin{equation}
 \frac{\partial E}{\partial \omega_{ji}}  = \frac{\partial E}{\partial y_i} \underbrace{\frac{\partial y_i}{\partial v_i}}_{\Theta^\prime} \frac{\partial v_i}{\partial \omega_{ji}}
 \end{equation}
 wo $\Theta^\prime$ einfach die Ableitung der nicht-linearen Aktivierungsfunktion $\Theta$ ist. Das heißt, dass der Fehler für die
 Ausgangsschicht jetzt folgendermaßen ausgerechnet wird:
\begin{equation}
  \Delta\omega_{ji} = \mu \cdot \Theta^\prime(y_i) \cdot y_j \cdot e_i 
\end{equation}
und der interne Fehler so:
\begin{equation}
\Delta\omega_{kj} = \mu \cdot \Theta^\prime(y_j) \cdot y_k \cdot \underbrace{\sum_i e_i \omega_{ji}}_{e_j} 
\end{equation}

Strikt muss die nicht-lineare Funktion $\Theta$ differenzierbar
sein. Es hat sich aber herausgestellt, dass der Einweggleichrichter
(Rectifiying Linear Unit = ReLU)
\begin{equation}
  \Theta(v) =
  \begin{cases}
    0, & \text{falls}\ v < 0 \\
    v, & \text{ansonsten}
  \end{cases}
\end{equation}
für viele Klassifizierungen am besten
ist. Dessen Ableitung hat keine eindeutige Lösung am Ursprung so dass man dort
entscheiden muss, ob sie eins oder Null ist. Andere populäre nicht-lineare
Funktionen sind $\Theta(v)=\tanh(v)$ oder die sigmoide Funktion: $\Theta(v)=\frac{1}{1+e^{-v}}$.




\end{document}
